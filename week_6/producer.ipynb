{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9425eef5",
   "metadata": {},
   "source": [
    "## Installing Kafka Manually on your Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a874186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from kafka import KafkaProducer\n",
    "from time import sleep\n",
    "from json import dumps\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49623ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-18 09:21:24--  https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.109.29, 52.216.218.144, 52.216.246.22, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.109.29|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12322 (12K) [application/octet-stream]\n",
      "Saving to: ‘taxi+_zone_lookup.csv’\n",
      "\n",
      "taxi+_zone_lookup.c 100%[===================>]  12.03K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-03-18 09:21:24 (101 MB/s) - ‘taxi+_zone_lookup.csv’ saved [12322/12322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv -O taxi+_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2afd1a0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoBrokersAvailable",
     "evalue": "NoBrokersAvailable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoBrokersAvailable\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_595682/1005490021.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m producer = KafkaProducer(bootstrap_servers=['localhost:9092'], #change ip here\n\u001b[0m\u001b[1;32m      2\u001b[0m                          \u001b[0mvalue_serializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                          dumps(x).encode('utf-8'))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/kafka/producer/kafka.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **configs)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         client = KafkaClient(metrics=self._metrics, metric_group_prefix='producer',\n\u001b[0m\u001b[1;32m    382\u001b[0m                              \u001b[0mwakeup_timeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_block_ms'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                              **self.config)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/kafka/client_async.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **configs)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'api_version'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mcheck_timeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'api_version_auto_timeout_ms'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'api_version'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_can_bootstrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/kafka/client_async.py\u001b[0m in \u001b[0;36mcheck_version\u001b[0;34m(self, node_id, timeout, strict)\u001b[0m\n\u001b[1;32m    898\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtry_node\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoBrokersAvailable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtry_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtry_node\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoBrokersAvailable\u001b[0m: NoBrokersAvailable"
     ]
    }
   ],
   "source": [
    "producer = KafkaProducer(bootstrap_servers=['localhost:9092'], #change ip here\n",
    "                         value_serializer=lambda x: \n",
    "                         dumps(x).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7d7df79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<kafka.producer.future.FutureRecordMetadata at 0x7f0e3aabacd0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "producer.send('demo_test', value={'surnasdasdame':'paulnwosu'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc0eb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c10e6f7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_595682/1261319607.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdict_stock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"records\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mproducer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'demo_test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict_stock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    dict_stock = df.sample(1).to_dict(orient=\"records\")[0]\n",
    "    producer.send('demo_test', value=dict_stock)\n",
    "    sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974eda4a",
   "metadata": {},
   "source": [
    "## Using Confluent Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b79cac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ccloud_config(config_file):\n",
    "    conf = {}\n",
    "    with open(config_file) as fh:\n",
    "        for line in fh:\n",
    "            line = line.strip()\n",
    "            if len(line) != 0 and line[0] != \"#\":\n",
    "                parameter, value = line.strip().split('=', 1)\n",
    "                conf[parameter] = value.strip()\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10412d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%4|1679137665.960|CONFWARN|rdkafka#producer-2| [thrd:app]: Configuration property session.timeout.ms is a consumer property and will be ignored by this producer instance\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Producer\n",
    "producer = Producer(read_ccloud_config(\"client.properties\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b3dba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "producer.produce(\"readcsv\", value='loverboy2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0364efd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Producer:\n",
      "\n",
      "class Producer(builtins.object)\n",
      " |  Asynchronous Kafka Producer\n",
      " |  \n",
      " |  .. py:function:: Producer(config)\n",
      " |  \n",
      " |    :param dict config: Configuration properties. At a minimum ``bootstrap.servers`` **should** be set\n",
      " |  \n",
      " |    Create a new Producer instance using the provided configuration dict.\n",
      " |  \n",
      " |  \n",
      " |  .. py:function:: __len__(self)\n",
      " |  \n",
      " |    Producer implements __len__ that can be used as len(producer) to obtain number of messages waiting.\n",
      " |    :returns: Number of messages and Kafka protocol requests waiting to be delivered to broker.\n",
      " |    :rtype: int\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __bool__(self, /)\n",
      " |      True if self else False\n",
      " |  \n",
      " |  __init__(self, /, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  abort_transaction(...)\n",
      " |      .. py:function:: abort_transaction([timeout])\n",
      " |      \n",
      " |      Aborts the current transaction.\n",
      " |      This function should also be used to recover from non-fatal\n",
      " |      abortable transaction errors when KafkaError.txn_requires_abort()\n",
      " |      is True.\n",
      " |      \n",
      " |      Any outstanding messages will be purged and fail with\n",
      " |      _PURGE_INFLIGHT or _PURGE_QUEUE.\n",
      " |      \n",
      " |      Note: This function will block until all outstanding messages\n",
      " |      are purged and the transaction abort request has been\n",
      " |      successfully handled by the transaction coordinator, or until\n",
      " |      the timeout expires, which ever comes first. On timeout the\n",
      " |      application may call the function again.\n",
      " |      \n",
      " |      Note: Will automatically call purge() and flush()  to ensure\n",
      " |      all queued and in-flight messages are purged before attempting\n",
      " |      to abort the transaction.\n",
      " |      \n",
      " |      :param float timeout: The maximum amount of time to block\n",
      " |           waiting for transaction to abort in seconds.\n",
      " |      \n",
      " |      :raises: KafkaError: Use exc.args[0].retriable() to check if the\n",
      " |               operation may be retried.\n",
      " |               Treat any other error as a fatal error.\n",
      " |  \n",
      " |  begin_transaction(...)\n",
      " |      .. py:function:: begin_transaction()\n",
      " |      \n",
      " |      Begin a new transaction.\n",
      " |      \n",
      " |      init_transactions() must have been called successfully (once)\n",
      " |      before this function is called.\n",
      " |      \n",
      " |      Any messages produced or offsets sent to a transaction, after\n",
      " |      the successful return of this function will be part of the\n",
      " |      transaction and committed or aborted atomically.\n",
      " |      \n",
      " |      Complete the transaction by calling commit_transaction() or\n",
      " |      Abort the transaction by calling abort_transaction().\n",
      " |      \n",
      " |      :raises: KafkaError: Use exc.args[0].retriable() to check if the\n",
      " |                           operation may be retried, else treat the\n",
      " |                           error as a fatal error.\n",
      " |  \n",
      " |  commit_transaction(...)\n",
      " |      .. py:function:: commit_transaction([timeout])\n",
      " |      \n",
      " |      Commmit the current transaction.\n",
      " |      Any outstanding messages will be flushed (delivered) before\n",
      " |      actually committing the transaction.\n",
      " |      \n",
      " |      If any of the outstanding messages fail permanently the current\n",
      " |      transaction will enter the abortable error state and this\n",
      " |      function will return an abortable error, in this case the\n",
      " |      application must call abort_transaction() before attempting\n",
      " |      a new transaction with begin_transaction().\n",
      " |      \n",
      " |      Note: This function will block until all outstanding messages\n",
      " |      are delivered and the transaction commit request has been\n",
      " |      successfully handled by the transaction coordinator, or until\n",
      " |      the timeout expires, which ever comes first. On timeout the\n",
      " |      application may call the function again.\n",
      " |      \n",
      " |      Note: Will automatically call flush() to ensure all queued\n",
      " |      messages are delivered before attempting to commit the\n",
      " |      transaction. Delivery reports and other callbacks may thus be\n",
      " |      triggered from this method.\n",
      " |      \n",
      " |      :param float timeout: The amount of time to block in seconds.\n",
      " |      \n",
      " |      :raises: KafkaError: Use exc.args[0].retriable() to check if the\n",
      " |               operation may be retried, or\n",
      " |               exc.args[0].txn_requires_abort() if the current\n",
      " |               transaction has failed and must be aborted by calling\n",
      " |               abort_transaction() and then start a new transaction\n",
      " |               with begin_transaction().\n",
      " |               Treat any other error as a fatal error.\n",
      " |  \n",
      " |  flush(...)\n",
      " |      .. py:function:: flush([timeout])\n",
      " |      \n",
      " |         Wait for all messages in the Producer queue to be delivered.\n",
      " |         This is a convenience method that calls :py:func:`poll()` until :py:func:`len()` is zero or the optional timeout elapses.\n",
      " |      \n",
      " |        :param: float timeout: Maximum time to block (requires librdkafka >= v0.9.4). (Seconds)\n",
      " |        :returns: Number of messages still in queue.\n",
      " |      \n",
      " |      .. note:: See :py:func:`poll()` for a description on what callbacks may be triggered.\n",
      " |  \n",
      " |  init_transactions(...)\n",
      " |      .. py:function: init_transactions([timeout])\n",
      " |      \n",
      " |      Initializes transactions for the producer instance.\n",
      " |      \n",
      " |      This function ensures any transactions initiated by previous\n",
      " |      instances of the producer with the same `transactional.id` are\n",
      " |      completed.\n",
      " |      If the previous instance failed with a transaction in progress\n",
      " |      the previous transaction will be aborted.\n",
      " |      This function needs to be called before any other transactional\n",
      " |      or produce functions are called when the `transactional.id` is\n",
      " |      configured.\n",
      " |      \n",
      " |      If the last transaction had begun completion (following\n",
      " |      transaction commit) but not yet finished, this function will\n",
      " |      await the previous transaction's completion.\n",
      " |      \n",
      " |      When any previous transactions have been fenced this function\n",
      " |      will acquire the internal producer id and epoch, used in all\n",
      " |      future transactional messages issued by this producer instance.\n",
      " |      \n",
      " |      Upon successful return from this function the application has to\n",
      " |      perform at least one of the following operations within \n",
      " |      `transaction.timeout.ms` to avoid timing out the transaction\n",
      " |      on the broker:\n",
      " |      * produce() (et.al)\n",
      " |      * send_offsets_to_transaction()\n",
      " |      * commit_transaction()\n",
      " |      * abort_transaction()\n",
      " |      \n",
      " |      :param float timeout: Maximum time to block in seconds.\n",
      " |      \n",
      " |      :raises: KafkaError: Use exc.args[0].retriable() to check if the\n",
      " |                           operation may be retried, else treat the\n",
      " |                           error as a fatal error.\n",
      " |  \n",
      " |  list_topics(...)\n",
      " |      .. py:function:: list_topics([topic=None], [timeout=-1])\n",
      " |      \n",
      " |      Request metadata from the cluster.\n",
      " |      This method provides the same information as  listTopics(), describeTopics() and describeCluster() in  the Java Admin client.\n",
      " |      \n",
      " |      :param str topic: If specified, only request information about this topic, else return results for all topics in cluster. Warning: If auto.create.topics.enable is set to true on the broker and an unknown topic is specified, it will be created.\n",
      " |      :param float timeout: The maximum response time before timing out, or -1 for infinite timeout.\n",
      " |      :rtype: ClusterMetadata\n",
      " |      :raises: KafkaException\n",
      " |  \n",
      " |  poll(...)\n",
      " |      .. py:function:: poll([timeout])\n",
      " |      \n",
      " |      Polls the producer for events and calls the corresponding callbacks (if registered).\n",
      " |      \n",
      " |      Callbacks:\n",
      " |      \n",
      " |      - ``on_delivery`` callbacks from :py:func:`produce()`\n",
      " |      - ...\n",
      " |      \n",
      " |      :param float timeout: Maximum time to block waiting for events. (Seconds)\n",
      " |      :returns: Number of events processed (callbacks served)\n",
      " |      :rtype: int\n",
      " |  \n",
      " |  produce(...)\n",
      " |      .. py:function:: produce(topic, [value], [key], [partition], [on_delivery], [timestamp], [headers])\n",
      " |      \n",
      " |      Produce message to topic.\n",
      " |      This is an asynchronous operation, an application may use the ``callback`` (alias ``on_delivery``) argument to pass a function (or lambda) that will be called from :py:func:`poll()` when the message has been successfully delivered or permanently fails delivery.\n",
      " |      \n",
      " |      Currently message headers are not supported on the message returned to the callback. The ``msg.headers()`` will return None even if the original message had headers set.\n",
      " |      \n",
      " |      :param str topic: Topic to produce message to\n",
      " |      :param str|bytes value: Message payload\n",
      " |      :param str|bytes key: Message key\n",
      " |      :param int partition: Partition to produce to, else uses the configured built-in partitioner.\n",
      " |      :param func on_delivery(err,msg): Delivery report callback to call (from :py:func:`poll()` or :py:func:`flush()`) on successful or failed delivery\n",
      " |      :param int timestamp: Message timestamp (CreateTime) in milliseconds since epoch UTC (requires librdkafka >= v0.9.4, api.version.request=true, and broker >= 0.10.0.0). Default value is current time.\n",
      " |      \n",
      " |      :param dict|list headers: Message headers to set on the message. The header key must be a string while the value must be binary, unicode or None. Accepts a list of (key,value) or a dict. (Requires librdkafka >= v0.11.4 and broker version >= 0.11.0.0)\n",
      " |      :rtype: None\n",
      " |      :raises BufferError: if the internal producer message queue is full (``queue.buffering.max.messages`` exceeded)\n",
      " |      :raises KafkaException: for other errors, see exception code\n",
      " |      :raises NotImplementedError: if timestamp is specified without underlying library support.\n",
      " |  \n",
      " |  purge(...)\n",
      " |      .. py:function:: purge([in_queue=True], [in_flight=True], [blocking=True])\n",
      " |      \n",
      " |       Purge messages currently handled by the producer instance.\n",
      " |       The application will need to call poll() or flush() afterwards to serve the delivery report callbacks of the purged messages.\n",
      " |      \n",
      " |      :param: bool in_queue: Purge messages from internal queues. By default, true.\n",
      " |      :param: bool in_flight: Purge messages in flight to or from the broker. By default, true.\n",
      " |      :param: bool blocking: If set to False, will not wait on background thread queue purging to finish. By default, true.\n",
      " |  \n",
      " |  send_offsets_to_transaction(...)\n",
      " |      .. py:function:: send_offsets_to_transaction(positions, group_metadata, [timeout])\n",
      " |      \n",
      " |      Sends a list of topic partition offsets to the consumer group\n",
      " |      coordinator for group_metadata and marks the offsets as part\n",
      " |      of the current transaction.\n",
      " |      These offsets will be considered committed only if the\n",
      " |      transaction is committed successfully.\n",
      " |      \n",
      " |      The offsets should be the next message your application will\n",
      " |      consume, i.e., the last processed message's offset + 1 for each\n",
      " |      partition.\n",
      " |      Either track the offsets manually during processing or use\n",
      " |      consumer.position() (on the consumer) to get the current offsets\n",
      " |      for the partitions assigned to the consumer.\n",
      " |      \n",
      " |      Use this method at the end of a consume-transform-produce loop\n",
      " |      prior to committing the transaction with commit_transaction().\n",
      " |      \n",
      " |      Note: The consumer must disable auto commits\n",
      " |            (set `enable.auto.commit` to false on the consumer).\n",
      " |      \n",
      " |      Note: Logical and invalid offsets (e.g., OFFSET_INVALID) in\n",
      " |      offsets will be ignored. If there are no valid offsets in\n",
      " |      offsets the function will return successfully and no action\n",
      " |      will be taken.\n",
      " |      \n",
      " |      :param list(TopicPartition) offsets: current consumer/processing\n",
      " |                                           position(offsets) for the\n",
      " |                                           list of partitions.\n",
      " |      :param object group_metadata: consumer group metadata retrieved\n",
      " |                                    from the input consumer's\n",
      " |                                    get_consumer_group_metadata().\n",
      " |      :param float timeout: Amount of time to block in seconds.\n",
      " |      \n",
      " |      :raises: KafkaError: Use exc.args[0].retriable() to check if the\n",
      " |               operation may be retried, or\n",
      " |               exc.args[0].txn_requires_abort() if the current\n",
      " |               transaction has failed and must be aborted by calling\n",
      " |               abort_transaction() and then start a new transaction\n",
      " |               with begin_transaction().\n",
      " |               Treat any other error as a fatal error.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Producer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5852176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
